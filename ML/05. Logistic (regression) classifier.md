# 5. Logistic (regression) classification

* Logistic Classification
  * Classification ì¤‘ ê°€ìž¥ ì •í™•ë„ê°€ ë†’ì€ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìžˆë‹¤.
  * ë‰´ëŸ´ ë„¤í¬ì›Œí¬ì™€ DeepLearning ì— ì¤‘ìš”í•œ Component



* Classification - Binary : 0, 1 Encoding
  * Spam Detection : Spam (1)  or Ham (0)
  * Facebook feed : Show (1) or Hide (0)
  * Credit Card Fraudulent Transaction detection : legitimate (1) / fraud (0)



* Pass (1) / Fail (0) based on study hours
  * ì‹œê°„ì— ë”°ë¥¸ í•©ê²© ê²°ê³¼ê°€ Pass/Fail ë¡œ ë‚˜ë‰¨



* Linear Regression ì„ ì´ìš©

  * Pass ì™€ Failì„ ë‚˜ë‰˜ëŠ” ì‹œê°„ì„ ìž¡ì•„ Linear Model ì˜ ê°€ì„¤ ì„¤ì • ê°€ëŠ¥

    ![Classifiatioin with linearRegressionGraph](./imgs/05_classificatioin_linear_regression.png)

  * ë¬¸ì œì 

    1. 50ì‹œê°„ ê³¼ ê°™ì€ í° ìž…ë ¥ê°’ì— ëŒ€í•´ì„œë„ 1ì˜ ê°’ìœ¼ë¡œ ê²°ê³¼ê°€ ë‚˜ì™€ì•¼í•¨ìœ¼ë¡œ ê¸°ìš¸ê¸°ì— ì˜í•´ í•©/ë¶ˆ íŒë‹¨ì˜ ê¸°ì¤€ì´ ë³€í™”í•˜ì—¬ í•©ê²© ë°ì´í„°ê°€ ë¶ˆí•©ê²©ìœ¼ë¡œ íŒë‹¨ ë  ìˆ˜ ìžˆë‹¤.

       ![error1](./imgs/05_linear_regression_error1.png)

    2. 'H(x) = Wx + b' ëª¨ë¸ì€ 1 ì´ìƒì˜ ê°’ì´ ê²°ê³¼ë¡œ ë‚˜ì˜¬ ìˆ˜ ìžˆë‹¤. 
       - í•™ìŠµ ë°ì´í„° x = [1, 2, 5, 10] => ëª¨ë¸ W=0.5
       - x = 100 -> y = 50 >>>> 1



* Logistic/Sigmoid fucntion
	
  * 0ê³¼ 1ì˜ ê°’ì„ ê°–ëŠ” í•¨ìˆ˜í˜•ì´ í•„ìš”í•¨
  	
* $$
    g(z)= \frac{1}{1+e^{-z}}
  $$
  
    ![logistic/sigmoid function](./imgs/05_logistic:sigmoid_function.png)
  
    * Sigmod : Curvered in two directions, like the letter 'S', or the Greek sigma
  
    * 1 ê³¼ 0 ì‚¬ì´ì˜ ê°’ë§Œ ê°€ì§€ê²Œ ë¨
  
  * $$
    H(x) = g(z)
    $$
  
  * $$
    z = Wx
    $$
  
  * $$
    then, H(X) = \frac{1}{1+e^{-W^{T}X}}
    $$



* Cost function

  * $$
    cost(W, b) = \frac{1}{m} \sum _{i=1}^{m}(H(x^{i})-y^{i})^{2}\ when\ H(x) = Wx + b
    $$

    * Linear regression ì˜ ê²½ìš° cost function ì€ convex í˜•íƒœ

  * í•˜ì§€ë§Œ, 
    $$
    then, H(X) = \frac{1}{1+e^{-W^{T}X}}
    $$
    ì˜ ê²½ìš°, ê¸°ìš¸ê¸°ê°€ ì¼ì •í•œ ê²½í–¥ì„±ì„ ë³´ì´ì§€ ì•Šê¸°ë•Œë¬¸ì— convex í˜•íƒœê°€ ì•„ë‹˜

    * ê¸°ìš¸ê¸°ê°€ 0ì¸ ë¶€ë¶„ìœ¼ë¡œ ìµœì €ì ì„ ì°¾ì„ ì‹œ, ì—¬ëŸ¬ êµ¬ê°„ì´ ë‚˜íƒ€ë‚˜ê²Œ ë¨

      ![non-convex graph](./imgs/05_nonconvex_graph.png)

    * Gradient decsent algorithm ì‚¬ìš© ë¶ˆê°€ëŠ¥



* New cost function for logistic

  * $$
    cost(W) = \frac{1}{m} \sum c(H(x), y)
    $$

  * $$
    c(H(x), y) = \begin{Bmatrix} -log(H(x)) : y = 1\\-log(1-H(x)) :y=0\end{Bmatrix}
    $$

    * c(H(x), y) í•¨ìˆ˜ëŠ” yì˜ ê²½ìš°ì— ë”°ë¼ í•¨ìˆ˜ë¥¼ ì •ì˜
  
  
  
* Understanding cost function

  * $$
    H(x) = \frac {1}{1+e^{-z}}
    $$

    * Sigmod function ì˜ ëª¨ì–‘ìœ¼ë¡œ ë‘ì—ˆì„ë•Œ, exponential term ì´ í¬í•¨ë¨
    * ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•˜ì—¬ log í•¨ìˆ˜ë¥¼ ì´ìš©

  * log fucntion

    * $$
      g(z) = -log(z)
      $$

      ![logFunction1](./imgs/05_log_function_1.png)

      * z = 0 -> g(0) = âˆž
      * z = 1 -> g(1) = 0

    * $$
      g(z) = -log(1-z)
      $$

      ![logFunction2](./imgs/05_log_function_2.png)

      * z = 0 -> g(0) = 0
      * z = 1 -> g(1) = âˆž

  * log function ì ìš©

    * y = 1

      * $$
        H(x) = 0, \ âž\ cost(1) = âˆž
        $$

      * $$
        H(x) = 1, \ âž\ cost(1) = 0
        $$

    * y = 0
    
      * $$
        H(x) = 0, \ âž\ cost(1) = 0
        $$
    
      * $$
        H(x) = 1, \ âž\ cost(1) = âˆž
        $$
      
    * ë‘ ê·¸ëž˜í”„ë¥¼ í•©ì³¤ì„ ê²½ìš°, convex function í˜•íƒœë¡œ ì´ìƒì ì´ê²Œ ëœë‹¤




* Cost Function ì •ë¦¬

  * $$
    cost(W) = \frac{1}{m} \sum{} c(H(x), y)
    $$

    

  * $$
    c(H(x), y) = \begin{Bmatrix} -log(H(x)) : y = 1\\-log(1-H(x)) :y=0\end{Bmatrix}
    $$

  

  * ìµœì¢…

  * $$
    c(H(x), y) = y \ log(H(x)) - (1-y)log(1-H(x))
    $$

    



* Minimize cost - Gradient Decent Algorithm

  * $$
    cost(W) = -\frac {1}{m}\sum y\ log(H(x))+(1-y)log(1-H(x))
    $$

    

  * $$
    W := W - ê­¤\frac{ðœ•}{ðœ•W}cost(W)
    $$

    





* Logistic (regression) classifier ì˜ Tensorflow êµ¬í˜„

  * Logistic Regression

    * $$
      H(x) = \frac{1}{1+e^{-W^{T}}X}
      $$

    * $$
      cost(W) = -\frac{1}{m}\sum y \ log(H(x))+(1-y)(log(1-H(x)))
      $$

    * $$
      W := W - ê­¤\frac{ðœ•}{ðœ•W}cost(W)
      $$

      * cost ê°’ì„ ìž‘ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” W ë¥¼ êµ¬í•˜ëŠ”ê²ƒì´ í•™ìŠµê³¼ì •

  ```python
  import tensorflow as tf
  
  x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]
  y_data = [[0], [0], [0], [1], [1], [1]]
  
  # placeholders for a tensor that will be always fed.
  X = tf.placeholder(tf.float32, shape=[None, 2])
  Y = tf.placeholder(tf.float32, shape=[None, 1])
  
  W = tf.Variable(tf.random_normal([2, 1]), name='weight')
  b = tf.Variable(tf.random_normal([1]), name='bias')
  
  # Hypothesis using sigmod: tf.div(1., 1. + tf.exp(tf.matmul(X, W) + b))
  # H(x) = Wx + b í˜•íƒœë¥¼ tensorflow ë‚´ìž¥ sigmoid í•¨ìˆ˜ì— ë„ì›€ì„ ë°›ì•„ sigmoid ëª¨ì–‘ìœ¼ë¡œ ë²ˆí˜•
  hypothesis = tf.sigmoid(tf.matmul(X, W)+b)
  
  # cost/Loss function
  # tf.reduce_mean() : í‰ê· ê°’
  cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))
  
  # costì˜ ìµœì €ê°’ì„ ì°¾ê¸°ìœ„í•´ í•™ìŠµí•˜ëŠ” ê·¸ëž˜í”„
  train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)
  
  # Accuracy computation
  # True if hypothesis > 0.5 else False
  # tf.cast(): True ë©´ 1, Fasle ë©´ 0 return
  predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
  
  # ì˜ˆì¸¡ëœ ê°’ê³¼ ì‹¤ì œ Y ê°’ì„ ë¹„êµ
  accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))
  
  # Launch graph
  with tf.Session() as sess:
      
      # initialize TensorFlow variables
      sess.run(tf.global_variables_initializer())
      
      print("# step, cost")
      for step in range(10001):
          cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y:y_data})
          
  
          if step % 2000 == 0:
              print(step, cost_val)
      
      # Accuracy report
      h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})
      print("\nHypothesis:\n", h, "\nCorrect (Y):\n", c, "\nAccuracy: ", a)
  ```

  * ê²°ê³¼

    ![result](./imgs/05_tensor_result.png)

    

    

    

* Numpy ì´ìš© ì˜ˆì œ

  ```python
  import tensorflow as tf
  import numpy as np
  
  xy = np.loadtxt('05_data-03-diabetes.csv', delimiter=",", dtype=np.float32)
  
  x_data = xy[:, 0:-1]
  y_data = xy[:, [-1]]
  
  # placeholder for a tensor that will be always fed.
  X = tf.placeholder(tf.float32, shape=[None, 8])
  Y = tf.placeholder(tf.float32, shape=[None, 1])
  
  W = tf.Variable(tf.random_normal([8, 1]), name="weight")
  b = tf.Variable(tf.random_normal([1]), name="bias")
  
  # Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))
  hypothesis = tf.sigmoid(tf.matmul(X, W)+b)
  
  # cost/loss function
  cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))
  train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)
  
  # Accuracy computation
  # True if hypothesis > 0.5 else False
  predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
  accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))
  
  # Launch graph
  with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())
      
      feed = {X: x_data, Y: y_data}
      
      for step in range(10001):
          sess.run(train, feed_dict = feed)
          
          if step % 2000 == 0:
              print(step, sess.run(cost, feed_dict=feed))
              
      # Accuracy report
      h, c, a = sess.run([hypothesis, predicted, accuracy] ,feed_dict=feed)
      print("\nHypothesis:\n", h, "\nCorrect (Y):\n", c, "\nAccuracy: \n", a)
  ```

  * ê²°ê³¼
    * Accuracy :  0.7641634