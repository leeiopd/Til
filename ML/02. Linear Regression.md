# 02. Linear Regression

* Regression (data)

  * 학습 데이터

    | x    | y    |
    | ---- | ---- |
    | 1    | 1    |
    | 2    | 2    |
    | 3    | 3    |

  * (Linear) Hypothesis

    * 'Linear 한 모델이 우리 데이터에 적합할 것이다' 라고 가설을 세워서 학습 시키는것이 Linear Regression

    * 많은 현상들이 Linear Regression으로 설명 할 수 있는 경우가 많다.

    * 데이터에 적합한 Linear 선 그래프를 찾는것이 Linear Regression

      * 1차 함수라고 가설을 세움 -> H(x) = Wx + b

    * Cost (Loss) function - 특정 모델이 좋은가 판단

      * 실제 데이터와 모델의 거리를 측정

      * $$
        ( H(x) - y )^2
        $$

        

        * 양의 거리만큼을 비교 가능

        * 차이가 커질때의 패널티를 부가하여 적합한 모델을 찾을때 유리하도록 동작

        * $$
          cost = \frac{1}{m} \sum_{i=1}^m(H(x^{(i)})-y^{(i)})^2
          $$

          * ( (모든 데이터와 모델의 차) 의 제곱의 합 ) 의 평균

          * $$
            H(x) = Wx + b
            $$

          * 을 대입

        * $$
          cost (W, b)= \frac{1}{m} \sum_{i=1}^m(H(x^{(i)})-y^{(i)})^2
          $$

          * x : 예상
          * y : 실제 데이터
          * H : hypothesis (모델)

      * 결론

        * Minimize cost(W, b) 의 모델을 구하는 것이 학습의 목표



* TensorFlow 로 구현

  1. Build graph using TF operations
     * H(x) = Wx + b

  ```python
  # X and Y data
  x_train = [1, 2, 3]
  y_train = [1, 2, 3]
  
  W = tf.Variable(tf.random_normal([1]), name='weight')
  b = tf.Variable(tf.random_normal([1]), name='bias')
  
  # Our hypothesis Wx + b
  hypothesis = x_train * W + b
  
  # cost/loss function
  cost = tf.reduce_mean(tf.square(hypothesis - y_train))
  
  # GradientDescent - 최저값 찾기
  optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)
  ## minimize 하는 그래프를 생성 train으로 명명
  train = optimizer.minimize(cost)
  ```



​	2~3. Run/update graph and get results

```python
# Launch the graph in an sessions
sess = tf.Session()

# Initializes global variables in the graph
## tensorflow 의 varialble 을 사용하기 전 initialize 시켜줘야함
sess.run(tf.global_variables_initializer())

# Fit the line
## 2000번 시행
for step in range(2001):
  sess.run(train)
  ## 200번마다 출력
  if step % 200 == 0:
    print(step, sess.run(cost), sess.run(W), sess.run(b))
    

# -> 0 2.0870929 [0.00082074] [0.8060834]
# -> 200 3.9132447e-06 [0.9977024] [0.00522294]
# -> 400 1.4944911e-06 [0.99858016] [0.00322756]
# -> 600 5.7069957e-07 [0.99912256] [0.0019946]
# -> 800 2.1799976e-07 [0.9994576] [0.00123278]
# -> 1000 8.330509e-08 [0.9996648] [0.00076211]
# -> 1200 3.1897006e-08 [0.9997926] [0.00047143]
# -> 1400 1.22062715e-08 [0.99987155] [0.00029171]
# -> 1600 4.6798854e-09 [0.9999204] [0.0001806]
# -> 1800 1.805688e-09 [0.99995047] [0.00011218]
# -> 2000 6.932481e-10 [0.99996936] [6.939867e-05]
```



* Placeholder 으로 구현

  ```python
  x = tf.placeholder(tf.float[32], shape=[None])
  y = tf.placeholder(tf.float[32], shape=[None] )
  
  for step in range(2001):
    cost_val, w_val, b_val = sess.run([cost, W, b, train], feed_dic={X : [1, 2, 3], Y : [1, 2, 3]})
    
    if step % 200 == 0:
      print(step, cost_val, w_val, v_val)
  ```

  